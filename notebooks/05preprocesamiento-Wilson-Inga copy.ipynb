{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e92cd0550dcfd1",
   "metadata": {},
   "source": [
    "# Ejercicio 5: Modelo Probabilístico\n",
    "\n",
    "## Objetivo de la práctica\n",
    "- Aplicar paso a paso técnicas de preprocesamiento, evaluando el impacto de cada etapa en el número de tokens y en el vocabulario final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88982921c8872f8f",
   "metadata": {},
   "source": [
    "## Parte 0: Carga del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T15:51:10.347651Z",
     "start_time": "2025-05-28T15:51:07.548869Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "corpus = newsgroups.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6196ea9cb414396",
   "metadata": {},
   "source": [
    "## Parte 1: Tokenización\n",
    "\n",
    "### Actividad \n",
    "1. Tokeniza los documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfb0f2438c9c0144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wil_s\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized corpus contains 18846 documents.\n",
      "Tokenized corpus: [[['I', 'am', 'sure', 'some', 'bashers', 'of', 'Pens', 'fans', 'are', 'pretty', 'confused', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'posts', 'about', 'the', 'recent', 'Pens', 'massacre', 'of', 'the', 'Devils', '.'], ['Actually', ',', 'I', 'am', 'bit', 'puzzled', 'too', 'and', 'a', 'bit', 'relieved', '.'], ['However', ',', 'I', 'am', 'going', 'to', 'put', 'an', 'end', 'to', 'non-PIttsburghers', \"'\", 'relief', 'with', 'a', 'bit', 'of', 'praise', 'for', 'the', 'Pens', '.'], ['Man', ',', 'they', 'are', 'killing', 'those', 'Devils', 'worse', 'than', 'I', 'thought', '.'], ['Jagr', 'just', 'showed', 'you', 'why', 'he', 'is', 'much', 'better', 'than', 'his', 'regular', 'season', 'stats', '.'], ['He', 'is', 'also', 'a', 'lot', 'fo', 'fun', 'to', 'watch', 'in', 'the', 'playoffs', '.'], ['Bowman', 'should', 'let', 'JAgr', 'have', 'a', 'lot', 'of', 'fun', 'in', 'the', 'next', 'couple', 'of', 'games', 'since', 'the', 'Pens', 'are', 'going', 'to', 'beat', 'the', 'pulp', 'out', 'of', 'Jersey', 'anyway', '.'], ['I', 'was', 'very', 'disappointed', 'not', 'to', 'see', 'the', 'Islanders', 'lose', 'the', 'final', 'regular', 'season', 'game', '.'], ['PENS', 'RULE', '!', '!'], ['!']], [['My', 'brother', 'is', 'in', 'the', 'market', 'for', 'a', 'high-performance', 'video', 'card', 'that', 'supports', 'VESA', 'local', 'bus', 'with', '1-2MB', 'RAM', '.'], ['Does', 'anyone', 'have', 'suggestions/ideas', 'on', ':', '-', 'Diamond', 'Stealth', 'Pro', 'Local', 'Bus', '-', 'Orchid', 'Farenheit', '1280', '-', 'ATI', 'Graphics', 'Ultra', 'Pro', '-', 'Any', 'other', 'high-performance', 'VLB', 'card', 'Please', 'post', 'or', 'email', '.'], ['Thank', 'you', '!'], ['-', 'Matt']]]\n"
     ]
    }
   ],
   "source": [
    "# bibliotecas necesarias para tokenización\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Función para tokenizar el corpus\n",
    "def tokenize_corpus(corpus):\n",
    "    tokenized_corpus = []\n",
    "    for document in corpus:\n",
    "        sentences = sent_tokenize(document)\n",
    "        tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "        tokenized_corpus.append(tokenized_sentences)\n",
    "    return tokenized_corpus\n",
    "# Tokenizar el corpus\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "print(f\"Tokenized corpus contains {len(tokenized_corpus)} documents.\")\n",
    "print(\"Tokenized corpus:\", tokenized_corpus[:2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ecfc1e6638e9d2",
   "metadata": {},
   "source": [
    "## Parte 2: Normalización\n",
    "\n",
    "### Actividad \n",
    "1. Convierte todos los tokens a minúsculas.\n",
    "2. Elimina puntuación y símbolos no alfabéticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc67a424c6550fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased corpus: [[['i', 'am', 'sure', 'some', 'bashers', 'of', 'pens', 'fans', 'are', 'pretty', 'confused', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'posts', 'about', 'the', 'recent', 'pens', 'massacre', 'of', 'the', 'devils', '.'], ['actually', ',', 'i', 'am', 'bit', 'puzzled', 'too', 'and', 'a', 'bit', 'relieved', '.'], ['however', ',', 'i', 'am', 'going', 'to', 'put', 'an', 'end', 'to', 'non-pittsburghers', \"'\", 'relief', 'with', 'a', 'bit', 'of', 'praise', 'for', 'the', 'pens', '.'], ['man', ',', 'they', 'are', 'killing', 'those', 'devils', 'worse', 'than', 'i', 'thought', '.'], ['jagr', 'just', 'showed', 'you', 'why', 'he', 'is', 'much', 'better', 'than', 'his', 'regular', 'season', 'stats', '.'], ['he', 'is', 'also', 'a', 'lot', 'fo', 'fun', 'to', 'watch', 'in', 'the', 'playoffs', '.'], ['bowman', 'should', 'let', 'jagr', 'have', 'a', 'lot', 'of', 'fun', 'in', 'the', 'next', 'couple', 'of', 'games', 'since', 'the', 'pens', 'are', 'going', 'to', 'beat', 'the', 'pulp', 'out', 'of', 'jersey', 'anyway', '.'], ['i', 'was', 'very', 'disappointed', 'not', 'to', 'see', 'the', 'islanders', 'lose', 'the', 'final', 'regular', 'season', 'game', '.'], ['pens', 'rule', '!', '!'], ['!']], [['my', 'brother', 'is', 'in', 'the', 'market', 'for', 'a', 'high-performance', 'video', 'card', 'that', 'supports', 'vesa', 'local', 'bus', 'with', '1-2mb', 'ram', '.'], ['does', 'anyone', 'have', 'suggestions/ideas', 'on', ':', '-', 'diamond', 'stealth', 'pro', 'local', 'bus', '-', 'orchid', 'farenheit', '1280', '-', 'ati', 'graphics', 'ultra', 'pro', '-', 'any', 'other', 'high-performance', 'vlb', 'card', 'please', 'post', 'or', 'email', '.'], ['thank', 'you', '!'], ['-', 'matt']]]\n"
     ]
    }
   ],
   "source": [
    "#Convertir todos los tokens a minúsculas\n",
    "def lowercase_tokens(tokenized_corpus):\n",
    "    lowercase_corpus = []\n",
    "    for document in tokenized_corpus:\n",
    "        lowercase_document = [[word.lower() for word in sentence] for sentence in document]\n",
    "        lowercase_corpus.append(lowercase_document)\n",
    "    return lowercase_corpus\n",
    "# Convertir los tokens a minúsculas\n",
    "lowercase_corpus = lowercase_tokens(tokenized_corpus)\n",
    "print(\"Lowercased corpus:\", lowercase_corpus[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3576d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned corpus: [[['i', 'am', 'sure', 'some', 'bashers', 'of', 'pens', 'fans', 'are', 'pretty', 'confused', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'posts', 'about', 'the', 'recent', 'pens', 'massacre', 'of', 'the', 'devils'], ['actually', 'i', 'am', 'bit', 'puzzled', 'too', 'and', 'a', 'bit', 'relieved'], ['however', 'i', 'am', 'going', 'to', 'put', 'an', 'end', 'to', 'relief', 'with', 'a', 'bit', 'of', 'praise', 'for', 'the', 'pens'], ['man', 'they', 'are', 'killing', 'those', 'devils', 'worse', 'than', 'i', 'thought'], ['jagr', 'just', 'showed', 'you', 'why', 'he', 'is', 'much', 'better', 'than', 'his', 'regular', 'season', 'stats'], ['he', 'is', 'also', 'a', 'lot', 'fo', 'fun', 'to', 'watch', 'in', 'the', 'playoffs'], ['bowman', 'should', 'let', 'jagr', 'have', 'a', 'lot', 'of', 'fun', 'in', 'the', 'next', 'couple', 'of', 'games', 'since', 'the', 'pens', 'are', 'going', 'to', 'beat', 'the', 'pulp', 'out', 'of', 'jersey', 'anyway'], ['i', 'was', 'very', 'disappointed', 'not', 'to', 'see', 'the', 'islanders', 'lose', 'the', 'final', 'regular', 'season', 'game'], ['pens', 'rule'], []], [['my', 'brother', 'is', 'in', 'the', 'market', 'for', 'a', 'video', 'card', 'that', 'supports', 'vesa', 'local', 'bus', 'with', 'ram'], ['does', 'anyone', 'have', 'on', 'diamond', 'stealth', 'pro', 'local', 'bus', 'orchid', 'farenheit', 'ati', 'graphics', 'ultra', 'pro', 'any', 'other', 'vlb', 'card', 'please', 'post', 'or', 'email'], ['thank', 'you'], ['matt']]]\n"
     ]
    }
   ],
   "source": [
    "# Eliminar puntuación y símbolos no alfabéticos\n",
    "import string\n",
    "def remove_punctuation(tokenized_corpus):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    cleaned_corpus = []\n",
    "    for document in tokenized_corpus:\n",
    "        cleaned_document = [[word.translate(table) for word in sentence if word.isalpha()] for sentence in document]\n",
    "        cleaned_corpus.append(cleaned_document)\n",
    "    return cleaned_corpus\n",
    "# Eliminar puntuación y símbolos no alfabéticos\n",
    "cleaned_corpus = remove_punctuation(lowercase_corpus)\n",
    "print(\"Cleaned corpus:\", cleaned_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973153ad553db841",
   "metadata": {},
   "source": [
    "## Parte 3: Eliminación de Stopwords\n",
    "\n",
    "### Actividad \n",
    "1. Elimina las palabras vacías usando una lista estándar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "477c7bcd5c2d0391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wil_s\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered corpus: [[['sure', 'bashers', 'pens', 'fans', 'pretty', 'confused', 'lack', 'kind', 'posts', 'recent', 'pens', 'massacre', 'devils'], ['actually', 'bit', 'puzzled', 'bit', 'relieved'], ['however', 'going', 'put', 'end', 'relief', 'bit', 'praise', 'pens'], ['man', 'killing', 'devils', 'worse', 'thought'], ['jagr', 'showed', 'much', 'better', 'regular', 'season', 'stats'], ['also', 'lot', 'fo', 'fun', 'watch', 'playoffs'], ['bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'couple', 'games', 'since', 'pens', 'going', 'beat', 'pulp', 'jersey', 'anyway'], ['disappointed', 'see', 'islanders', 'lose', 'final', 'regular', 'season', 'game'], ['pens', 'rule'], []], [['brother', 'market', 'video', 'card', 'supports', 'vesa', 'local', 'bus', 'ram'], ['anyone', 'diamond', 'stealth', 'pro', 'local', 'bus', 'orchid', 'farenheit', 'ati', 'graphics', 'ultra', 'pro', 'vlb', 'card', 'please', 'post', 'email'], ['thank'], ['matt']]]\n"
     ]
    }
   ],
   "source": [
    "# Eliminar las palabras vacías (stopwords) usando lista estandar de NLTK\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "# Función para eliminar las palabras vacías (stopwords)\n",
    "def remove_stopwords(tokenized_corpus):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_corpus = []\n",
    "    for document in tokenized_corpus:\n",
    "        filtered_document = [[word for word in sentence if word not in stop_words] for sentence in document]\n",
    "        filtered_corpus.append(filtered_document)\n",
    "    return filtered_corpus\n",
    "# Eliminar las palabras vacías (stopwords)\n",
    "filtered_corpus = remove_stopwords(cleaned_corpus)\n",
    "print(\"Filtered corpus:\", filtered_corpus[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f090bfc7868f8",
   "metadata": {},
   "source": [
    "## Parte 4: Stemming o Lematización\n",
    "\n",
    "### Actividad\n",
    "1. Aplica stemming.\n",
    "2. Aplica lematización.\n",
    "3. Compara ambas técnicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd9ff693047bd948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed corpus: [[['sure', 'basher', 'pen', 'fan', 'pretti', 'confus', 'lack', 'kind', 'post', 'recent', 'pen', 'massacr', 'devil'], ['actual', 'bit', 'puzzl', 'bit', 'reliev'], ['howev', 'go', 'put', 'end', 'relief', 'bit', 'prais', 'pen'], ['man', 'kill', 'devil', 'wors', 'thought'], ['jagr', 'show', 'much', 'better', 'regular', 'season', 'stat'], ['also', 'lot', 'fo', 'fun', 'watch', 'playoff'], ['bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'coupl', 'game', 'sinc', 'pen', 'go', 'beat', 'pulp', 'jersey', 'anyway'], ['disappoint', 'see', 'island', 'lose', 'final', 'regular', 'season', 'game'], ['pen', 'rule'], []], [['brother', 'market', 'video', 'card', 'support', 'vesa', 'local', 'bu', 'ram'], ['anyon', 'diamond', 'stealth', 'pro', 'local', 'bu', 'orchid', 'farenheit', 'ati', 'graphic', 'ultra', 'pro', 'vlb', 'card', 'pleas', 'post', 'email'], ['thank'], ['matt']]]\n"
     ]
    }
   ],
   "source": [
    "# Aplicar stemming usando el algoritmo de Porter\n",
    "from nltk.stem import PorterStemmer\n",
    "def stem_tokens(tokenized_corpus):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_corpus = []\n",
    "    for document in tokenized_corpus:\n",
    "        stemmed_document = [[stemmer.stem(word) for word in sentence] for sentence in document]\n",
    "        stemmed_corpus.append(stemmed_document)\n",
    "    return stemmed_corpus\n",
    "# Aplicar stemming\n",
    "stemmed_corpus = stem_tokens(filtered_corpus)\n",
    "print(\"Stemmed corpus:\", stemmed_corpus[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52e90d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wil_s\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized corpus: [[['sure', 'bashers', 'pen', 'fan', 'pretty', 'confused', 'lack', 'kind', 'post', 'recent', 'pen', 'massacre', 'devil'], ['actually', 'bit', 'puzzled', 'bit', 'relieved'], ['however', 'going', 'put', 'end', 'relief', 'bit', 'praise', 'pen'], ['man', 'killing', 'devil', 'worse', 'thought'], ['jagr', 'showed', 'much', 'better', 'regular', 'season', 'stats'], ['also', 'lot', 'fo', 'fun', 'watch', 'playoff'], ['bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'couple', 'game', 'since', 'pen', 'going', 'beat', 'pulp', 'jersey', 'anyway'], ['disappointed', 'see', 'islander', 'lose', 'final', 'regular', 'season', 'game'], ['pen', 'rule'], []], [['brother', 'market', 'video', 'card', 'support', 'vesa', 'local', 'bus', 'ram'], ['anyone', 'diamond', 'stealth', 'pro', 'local', 'bus', 'orchid', 'farenheit', 'ati', 'graphic', 'ultra', 'pro', 'vlb', 'card', 'please', 'post', 'email'], ['thank'], ['matt']]]\n"
     ]
    }
   ],
   "source": [
    "# Aplicar lematización\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "def lemmatize_tokens(tokenized_corpus):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_corpus = []\n",
    "    for document in tokenized_corpus:\n",
    "        lemmatized_document = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in document]\n",
    "        lemmatized_corpus.append(lemmatized_document)\n",
    "    return lemmatized_corpus   \n",
    "# Aplicar lematización\n",
    "lemmatized_corpus = lemmatize_tokens(filtered_corpus)\n",
    "print(\"Lemmatized corpus:\", lemmatized_corpus[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05bda09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of tokenization steps:\n",
      "\n",
      "Document 1:\n",
      "Original: \n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "Tokenized: [['I', 'am', 'sure', 'some', 'bashers', 'of', 'Pens', 'fans', 'are', 'pretty', 'confused', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'posts', 'about', 'the', 'recent', 'Pens', 'massacre', 'of', 'the', 'Devils', '.'], ['Actually', ',', 'I', 'am', 'bit', 'puzzled', 'too', 'and', 'a', 'bit', 'relieved', '.'], ['However', ',', 'I', 'am', 'going', 'to', 'put', 'an', 'end', 'to', 'non-PIttsburghers', \"'\", 'relief', 'with', 'a', 'bit', 'of', 'praise', 'for', 'the', 'Pens', '.'], ['Man', ',', 'they', 'are', 'killing', 'those', 'Devils', 'worse', 'than', 'I', 'thought', '.'], ['Jagr', 'just', 'showed', 'you', 'why', 'he', 'is', 'much', 'better', 'than', 'his', 'regular', 'season', 'stats', '.'], ['He', 'is', 'also', 'a', 'lot', 'fo', 'fun', 'to', 'watch', 'in', 'the', 'playoffs', '.'], ['Bowman', 'should', 'let', 'JAgr', 'have', 'a', 'lot', 'of', 'fun', 'in', 'the', 'next', 'couple', 'of', 'games', 'since', 'the', 'Pens', 'are', 'going', 'to', 'beat', 'the', 'pulp', 'out', 'of', 'Jersey', 'anyway', '.'], ['I', 'was', 'very', 'disappointed', 'not', 'to', 'see', 'the', 'Islanders', 'lose', 'the', 'final', 'regular', 'season', 'game', '.'], ['PENS', 'RULE', '!', '!'], ['!']]\n",
      "Lowercased: [['i', 'am', 'sure', 'some', 'bashers', 'of', 'pens', 'fans', 'are', 'pretty', 'confused', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'posts', 'about', 'the', 'recent', 'pens', 'massacre', 'of', 'the', 'devils', '.'], ['actually', ',', 'i', 'am', 'bit', 'puzzled', 'too', 'and', 'a', 'bit', 'relieved', '.'], ['however', ',', 'i', 'am', 'going', 'to', 'put', 'an', 'end', 'to', 'non-pittsburghers', \"'\", 'relief', 'with', 'a', 'bit', 'of', 'praise', 'for', 'the', 'pens', '.'], ['man', ',', 'they', 'are', 'killing', 'those', 'devils', 'worse', 'than', 'i', 'thought', '.'], ['jagr', 'just', 'showed', 'you', 'why', 'he', 'is', 'much', 'better', 'than', 'his', 'regular', 'season', 'stats', '.'], ['he', 'is', 'also', 'a', 'lot', 'fo', 'fun', 'to', 'watch', 'in', 'the', 'playoffs', '.'], ['bowman', 'should', 'let', 'jagr', 'have', 'a', 'lot', 'of', 'fun', 'in', 'the', 'next', 'couple', 'of', 'games', 'since', 'the', 'pens', 'are', 'going', 'to', 'beat', 'the', 'pulp', 'out', 'of', 'jersey', 'anyway', '.'], ['i', 'was', 'very', 'disappointed', 'not', 'to', 'see', 'the', 'islanders', 'lose', 'the', 'final', 'regular', 'season', 'game', '.'], ['pens', 'rule', '!', '!'], ['!']]\n",
      "Cleaned: [['i', 'am', 'sure', 'some', 'bashers', 'of', 'pens', 'fans', 'are', 'pretty', 'confused', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'posts', 'about', 'the', 'recent', 'pens', 'massacre', 'of', 'the', 'devils'], ['actually', 'i', 'am', 'bit', 'puzzled', 'too', 'and', 'a', 'bit', 'relieved'], ['however', 'i', 'am', 'going', 'to', 'put', 'an', 'end', 'to', 'relief', 'with', 'a', 'bit', 'of', 'praise', 'for', 'the', 'pens'], ['man', 'they', 'are', 'killing', 'those', 'devils', 'worse', 'than', 'i', 'thought'], ['jagr', 'just', 'showed', 'you', 'why', 'he', 'is', 'much', 'better', 'than', 'his', 'regular', 'season', 'stats'], ['he', 'is', 'also', 'a', 'lot', 'fo', 'fun', 'to', 'watch', 'in', 'the', 'playoffs'], ['bowman', 'should', 'let', 'jagr', 'have', 'a', 'lot', 'of', 'fun', 'in', 'the', 'next', 'couple', 'of', 'games', 'since', 'the', 'pens', 'are', 'going', 'to', 'beat', 'the', 'pulp', 'out', 'of', 'jersey', 'anyway'], ['i', 'was', 'very', 'disappointed', 'not', 'to', 'see', 'the', 'islanders', 'lose', 'the', 'final', 'regular', 'season', 'game'], ['pens', 'rule'], []]\n",
      "Filtered: [['sure', 'bashers', 'pens', 'fans', 'pretty', 'confused', 'lack', 'kind', 'posts', 'recent', 'pens', 'massacre', 'devils'], ['actually', 'bit', 'puzzled', 'bit', 'relieved'], ['however', 'going', 'put', 'end', 'relief', 'bit', 'praise', 'pens'], ['man', 'killing', 'devils', 'worse', 'thought'], ['jagr', 'showed', 'much', 'better', 'regular', 'season', 'stats'], ['also', 'lot', 'fo', 'fun', 'watch', 'playoffs'], ['bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'couple', 'games', 'since', 'pens', 'going', 'beat', 'pulp', 'jersey', 'anyway'], ['disappointed', 'see', 'islanders', 'lose', 'final', 'regular', 'season', 'game'], ['pens', 'rule'], []]\n",
      "Stemmed: [['sure', 'basher', 'pen', 'fan', 'pretti', 'confus', 'lack', 'kind', 'post', 'recent', 'pen', 'massacr', 'devil'], ['actual', 'bit', 'puzzl', 'bit', 'reliev'], ['howev', 'go', 'put', 'end', 'relief', 'bit', 'prais', 'pen'], ['man', 'kill', 'devil', 'wors', 'thought'], ['jagr', 'show', 'much', 'better', 'regular', 'season', 'stat'], ['also', 'lot', 'fo', 'fun', 'watch', 'playoff'], ['bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'coupl', 'game', 'sinc', 'pen', 'go', 'beat', 'pulp', 'jersey', 'anyway'], ['disappoint', 'see', 'island', 'lose', 'final', 'regular', 'season', 'game'], ['pen', 'rule'], []]\n",
      "Lemmatized: [['sure', 'bashers', 'pen', 'fan', 'pretty', 'confused', 'lack', 'kind', 'post', 'recent', 'pen', 'massacre', 'devil'], ['actually', 'bit', 'puzzled', 'bit', 'relieved'], ['however', 'going', 'put', 'end', 'relief', 'bit', 'praise', 'pen'], ['man', 'killing', 'devil', 'worse', 'thought'], ['jagr', 'showed', 'much', 'better', 'regular', 'season', 'stats'], ['also', 'lot', 'fo', 'fun', 'watch', 'playoff'], ['bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'couple', 'game', 'since', 'pen', 'going', 'beat', 'pulp', 'jersey', 'anyway'], ['disappointed', 'see', 'islander', 'lose', 'final', 'regular', 'season', 'game'], ['pen', 'rule'], []]\n",
      "\n",
      "Document 2:\n",
      "Original: My brother is in the market for a high-performance video card that supports\n",
      "VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\n",
      "\n",
      "  - Diamond Stealth Pro Local Bus\n",
      "\n",
      "  - Orchid Farenheit 1280\n",
      "\n",
      "  - ATI Graphics Ultra Pro\n",
      "\n",
      "  - Any other high-performance VLB card\n",
      "\n",
      "\n",
      "Please post or email.  Thank you!\n",
      "\n",
      "  - Matt\n",
      "\n",
      "Tokenized: [['My', 'brother', 'is', 'in', 'the', 'market', 'for', 'a', 'high-performance', 'video', 'card', 'that', 'supports', 'VESA', 'local', 'bus', 'with', '1-2MB', 'RAM', '.'], ['Does', 'anyone', 'have', 'suggestions/ideas', 'on', ':', '-', 'Diamond', 'Stealth', 'Pro', 'Local', 'Bus', '-', 'Orchid', 'Farenheit', '1280', '-', 'ATI', 'Graphics', 'Ultra', 'Pro', '-', 'Any', 'other', 'high-performance', 'VLB', 'card', 'Please', 'post', 'or', 'email', '.'], ['Thank', 'you', '!'], ['-', 'Matt']]\n",
      "Lowercased: [['my', 'brother', 'is', 'in', 'the', 'market', 'for', 'a', 'high-performance', 'video', 'card', 'that', 'supports', 'vesa', 'local', 'bus', 'with', '1-2mb', 'ram', '.'], ['does', 'anyone', 'have', 'suggestions/ideas', 'on', ':', '-', 'diamond', 'stealth', 'pro', 'local', 'bus', '-', 'orchid', 'farenheit', '1280', '-', 'ati', 'graphics', 'ultra', 'pro', '-', 'any', 'other', 'high-performance', 'vlb', 'card', 'please', 'post', 'or', 'email', '.'], ['thank', 'you', '!'], ['-', 'matt']]\n",
      "Cleaned: [['my', 'brother', 'is', 'in', 'the', 'market', 'for', 'a', 'video', 'card', 'that', 'supports', 'vesa', 'local', 'bus', 'with', 'ram'], ['does', 'anyone', 'have', 'on', 'diamond', 'stealth', 'pro', 'local', 'bus', 'orchid', 'farenheit', 'ati', 'graphics', 'ultra', 'pro', 'any', 'other', 'vlb', 'card', 'please', 'post', 'or', 'email'], ['thank', 'you'], ['matt']]\n",
      "Filtered: [['brother', 'market', 'video', 'card', 'supports', 'vesa', 'local', 'bus', 'ram'], ['anyone', 'diamond', 'stealth', 'pro', 'local', 'bus', 'orchid', 'farenheit', 'ati', 'graphics', 'ultra', 'pro', 'vlb', 'card', 'please', 'post', 'email'], ['thank'], ['matt']]\n",
      "Stemmed: [['brother', 'market', 'video', 'card', 'support', 'vesa', 'local', 'bu', 'ram'], ['anyon', 'diamond', 'stealth', 'pro', 'local', 'bu', 'orchid', 'farenheit', 'ati', 'graphic', 'ultra', 'pro', 'vlb', 'card', 'pleas', 'post', 'email'], ['thank'], ['matt']]\n",
      "Lemmatized: [['brother', 'market', 'video', 'card', 'support', 'vesa', 'local', 'bus', 'ram'], ['anyone', 'diamond', 'stealth', 'pro', 'local', 'bus', 'orchid', 'farenheit', 'ati', 'graphic', 'ultra', 'pro', 'vlb', 'card', 'please', 'post', 'email'], ['thank'], ['matt']]\n"
     ]
    }
   ],
   "source": [
    "# Comparación de los resultados\n",
    "print(\"\\nComparison of tokenization steps:\")\n",
    "for i in range(2):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(\"Original:\", corpus[i])\n",
    "    print(\"Tokenized:\", tokenized_corpus[i])\n",
    "    print(\"Lowercased:\", lowercase_corpus[i])\n",
    "    print(\"Cleaned:\", cleaned_corpus[i])\n",
    "    print(\"Filtered:\", filtered_corpus[i])\n",
    "    print(\"Stemmed:\", stemmed_corpus[i])\n",
    "    print(\"Lemmatized:\", lemmatized_corpus[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
